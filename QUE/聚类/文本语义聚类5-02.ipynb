{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "\n",
    "import jieba\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.cluster import KMeans\n",
    "import os\n",
    "current_dir = os.getcwd()\n",
    "os.chdir(current_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "df = pd.read_excel(r\".\\df_query_2_512B.xlsx\")\n",
    "df_0 = df[df['similarity']<=0.75]\n",
    "df_0 = df_0.reset_index(drop=True)\n",
    "df_ff = df_0['food_feature']\n",
    "df_ff.to_csv(r'./模糊特征515.txt',index=False,header=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class KmeansClustering():\n",
    "    def __init__(self, stopwords_path=None):\n",
    "        self.stopwords = self.load_stopwords(stopwords_path)\n",
    "        self.vectorizer = CountVectorizer()\n",
    "        self.transformer = TfidfTransformer()\n",
    "\n",
    "    def load_stopwords(self, stopwords=None):\n",
    "        \"\"\"\n",
    "        加载停用词\n",
    "        :param stopwords:\n",
    "        :return:\n",
    "        \"\"\"\n",
    "        if stopwords:\n",
    "            with open(stopwords, 'r', encoding='utf-8') as f:\n",
    "                return [line.strip() for line in f]\n",
    "        else:\n",
    "            return []\n",
    "\n",
    "    def preprocess_data(self, corpus_path):\n",
    "        \"\"\"\n",
    "        文本预处理，每行一个文本\n",
    "        :param corpus_path:\n",
    "        :return:\n",
    "        \"\"\"\n",
    "        corpus = []\n",
    "        with open(corpus_path, 'r', encoding='utf-8') as f:\n",
    "            for line in f:\n",
    "                corpus.append(' '.join([word for word in jieba.lcut(line.strip()) if word not in self.stopwords]))\n",
    "        return corpus\n",
    "\n",
    "    def get_text_tfidf_matrix(self, corpus):\n",
    "        \"\"\"\n",
    "        获取tfidf矩阵\n",
    "        :param corpus:\n",
    "        :return:\n",
    "        \"\"\"\n",
    "        tfidf = self.transformer.fit_transform(self.vectorizer.fit_transform(corpus))\n",
    "\n",
    "        # 获取词袋中所有词语\n",
    "        # words = self.vectorizer.get_feature_names()\n",
    "\n",
    "        # 获取tfidf矩阵中权重\n",
    "        weights = tfidf.toarray()\n",
    "        return weights\n",
    "\n",
    "    def kmeans(self, corpus_path, n_clusters):\n",
    "        \"\"\"\n",
    "        KMeans文本聚类\n",
    "        :param corpus_path: 语料路径（每行一篇）,文章id从0开始\n",
    "        :param n_clusters: ：聚类类别数目\n",
    "        :return: {cluster_id1:[text_id1, text_id2]}\n",
    "        \"\"\"\n",
    "        corpus = self.preprocess_data(corpus_path)\n",
    "        weights = self.get_text_tfidf_matrix(corpus)\n",
    "\n",
    "        clf = KMeans(n_clusters=n_clusters)\n",
    "\n",
    "        # clf.fit(weights)\n",
    "\n",
    "        y = clf.fit_predict(weights)\n",
    "\n",
    "        # 中心点\n",
    "        # centers = clf.cluster_centers_\n",
    "\n",
    "        # 用来评估簇的个数是否合适,距离约小说明簇分得越好,选取临界点的簇的个数\n",
    "        # score = clf.inertia_\n",
    "\n",
    "        # 每个样本所属的簇\n",
    "        result = {}\n",
    "        for text_idx, label_idx in enumerate(y):\n",
    "            if label_idx not in result:\n",
    "                result[label_idx] = [text_idx]\n",
    "            else:\n",
    "                result[label_idx].append(text_idx)\n",
    "        return result\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Building prefix dict from the default dictionary ...\n",
      "Loading model from cache C:\\Users\\GUOFEN~1\\AppData\\Local\\Temp\\jieba.cache\n",
      "Loading model cost 0.441 seconds.\n",
      "Prefix dict has been built successfully.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{3: [0, 1, 2, 4, 5, 6, 9, 13, 14, 15, 16, 18, 20, 22, 24, 26, 28, 32, 36, 37, 47, 48, 50, 51, 52, 53, 55, 56, 57, 58, 59, 61, 62, 63, 64, 67, 68, 69, 72, 74, 75, 76, 77, 78, 79, 82, 83, 85, 86, 87, 88, 91, 92, 93, 94, 95, 96, 97, 98, 101, 102, 103, 104, 105, 107, 108, 110, 111, 114, 116, 117, 118, 120, 121, 122, 123], 4: [3, 8, 25, 31, 112], 7: [7, 21, 35, 42, 54, 60], 6: [10, 17, 34], 8: [11, 12], 1: [19, 29, 30, 43, 65, 66], 13: [23], 14: [27, 45], 0: [33, 38, 46], 2: [39, 40, 41, 44, 84, 90, 99, 109, 113, 115, 119], 5: [49, 71, 100, 124], 11: [70, 106], 9: [73], 10: [80, 89], 12: [81]}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\app\\anaconda\\Lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1429: UserWarning: KMeans is known to have a memory leak on Windows with MKL, when there are less chunks than available threads. You can avoid it by setting the environment variable OMP_NUM_THREADS=1.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "Kmeans = KmeansClustering()\n",
    "result = Kmeans.kmeans(r'./模糊特征515.txt', n_clusters=15)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_ff = pd.read_csv(r'./模糊特征515.txt',names=['food_feature'])\n",
    "df_ff['cluster_id'] = -1\n",
    "# 遍历聚类结果字典，标记每行属于哪个簇\n",
    "for cluster_id, indices in result.items():\n",
    "    df_ff.loc[indices, 'cluster_id'] = cluster_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_0 = pd.concat([df_ff,df_0[['Query_faiss','recommend']]], axis=1)\n",
    "df_0 = df_0[['food_feature','Query_faiss','recommend','cluster_id']].sort_values(by='cluster_id', ascending=True) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_0.to_excel(r'./聚类515.xlsx')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
