{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f73f8a53-e28a-40cf-8243-c9fc26213060",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "成功加载 3427 个菜品\n",
      "\n",
      "===== 评估结果 =====\n",
      "ASR得分: 1.0000\n",
      "DOM得分: 0.6667\n",
      "QUE得分: 0.5000\n",
      "总分: 0.8000\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import Levenshtein  # 用于计算编辑距离\n",
    "import pandas as pd\n",
    "import re\n",
    "from typing import List, Dict, Tuple\n",
    "\n",
    "class ELEAIEvaluator:\n",
    "    def __init__(self, weights=(0.5, 0.3, 0.2)):\n",
    "        \"\"\"\n",
    "        初始化评估器\n",
    "        \n",
    "        Args:\n",
    "            weights: ASR、DOM和QUE的权重，默认为(0.5, 0.3, 0.2)\n",
    "        \"\"\"\n",
    "        self.weights = weights\n",
    "        self.dish_database = set()  # 菜品数据库\n",
    "    \n",
    "    def load_dish_database(self, dish_file_path):\n",
    "        \"\"\"\n",
    "        加载菜品数据库\n",
    "        \n",
    "        Args:\n",
    "            dish_file_path: 菜品数据文件路径\n",
    "        \"\"\"\n",
    "        try:\n",
    "            df = pd.read_excel(dish_file_path)\n",
    "            self.dish_database = set(df['item'].str.strip().tolist())\n",
    "            print(f\"成功加载 {len(self.dish_database)} 个菜品\")\n",
    "        except Exception as e:\n",
    "            print(f\"加载菜品数据库失败: {e}\")\n",
    "    \n",
    "    def preprocess_text(self, text):\n",
    "        \"\"\"\n",
    "        预处理文本，去除标点符号\n",
    "        \n",
    "        Args:\n",
    "            text: 输入文本\n",
    "            \n",
    "        Returns:\n",
    "            处理后的文本\n",
    "        \"\"\"\n",
    "        if not isinstance(text, str):\n",
    "            return \"\"\n",
    "        # 去除标点符号\n",
    "        return re.sub(r'[^\\w\\s]', '', text)\n",
    "    \n",
    "    def calculate_cer(self, reference, hypothesis):\n",
    "        \"\"\"\n",
    "        计算字错率(CER)\n",
    "        \n",
    "        Args:\n",
    "            reference: 参考文本(标准答案)\n",
    "            hypothesis: 假设文本(模型输出)\n",
    "            \n",
    "        Returns:\n",
    "            字错率\n",
    "        \"\"\"\n",
    "        # 预处理文本，去除标点符号\n",
    "        reference = self.preprocess_text(reference)\n",
    "        hypothesis = self.preprocess_text(hypothesis)\n",
    "        \n",
    "        if len(reference) == 0:\n",
    "            return 1.0 if len(hypothesis) > 0 else 0.0\n",
    "        \n",
    "        # 计算编辑距离\n",
    "        edit_distance = Levenshtein.distance(reference, hypothesis)\n",
    "        \n",
    "        # 计算CER\n",
    "        cer = edit_distance / len(reference)\n",
    "        return cer\n",
    "    \n",
    "    def evaluate_asr(self, references, hypotheses):\n",
    "        \"\"\"\n",
    "        评估ASR性能\n",
    "        \n",
    "        Args:\n",
    "            references: 参考文本列表\n",
    "            hypotheses: 假设文本列表\n",
    "            \n",
    "        Returns:\n",
    "            ASR得分\n",
    "        \"\"\"\n",
    "        if len(references) != len(hypotheses):\n",
    "            raise ValueError(\"参考文本和假设文本数量不一致\")\n",
    "        \n",
    "        cers = []\n",
    "        for ref, hyp in zip(references, hypotheses):\n",
    "            cer = self.calculate_cer(ref, hyp)\n",
    "            cers.append(cer)\n",
    "        \n",
    "        # 计算ASR得分\n",
    "        asr_score = 1 - np.mean(cers)\n",
    "        return asr_score\n",
    "    \n",
    "    def evaluate_dom(self, true_labels, predicted_labels):\n",
    "        \"\"\"\n",
    "        评估领域分类(DOM)性能\n",
    "        \n",
    "        Args:\n",
    "            true_labels: 真实标签列表(0或1)\n",
    "            predicted_labels: 预测标签列表(0或1)\n",
    "            \n",
    "        Returns:\n",
    "            DOM得分\n",
    "        \"\"\"\n",
    "        if len(true_labels) != len(predicted_labels):\n",
    "            raise ValueError(\"真实标签和预测标签数量不一致\")\n",
    "        \n",
    "        # 计算准确率\n",
    "        correct = sum(1 for t, p in zip(true_labels, predicted_labels) if t == p)\n",
    "        dom_score = correct / len(true_labels)\n",
    "        \n",
    "        return dom_score\n",
    "    \n",
    "    def evaluate_que(self, queries, domain_predictions, model_scores):\n",
    "        \"\"\"\n",
    "        评估查询改写(QUE)性能\n",
    "        \n",
    "        Args:\n",
    "            queries: 改写后的查询列表\n",
    "            domain_predictions: 领域预测结果列表(0或1)\n",
    "            model_scores: 模型评分列表(0-5)\n",
    "            \n",
    "        Returns:\n",
    "            QUE得分\n",
    "        \"\"\"\n",
    "        if not (len(queries) == len(domain_predictions) == len(model_scores)):\n",
    "            raise ValueError(\"查询、领域预测和模型评分数量不一致\")\n",
    "        \n",
    "        # 提取预测为外卖领域的样本\n",
    "        valid_indices = [i for i, pred in enumerate(domain_predictions) if pred == 1]\n",
    "        \n",
    "        if not valid_indices:\n",
    "            return 0.0  # 没有预测为外卖领域的样本\n",
    "        \n",
    "        total_score = 0\n",
    "        for idx in valid_indices:\n",
    "            query = queries[idx]\n",
    "            score = model_scores[idx]\n",
    "            \n",
    "            # 检查菜品是否在菜品库中\n",
    "            dishes_in_query = self.extract_dishes(query)\n",
    "            q_i = 1 if any(dish in self.dish_database for dish in dishes_in_query) else 0\n",
    "            \n",
    "            # 计算单个查询的得分\n",
    "            query_score = (q_i * score) / 5\n",
    "            total_score += query_score\n",
    "        \n",
    "        # 计算QUE得分\n",
    "        que_score = total_score / len(valid_indices)\n",
    "        return que_score\n",
    "    \n",
    "    def extract_dishes(self, query):\n",
    "        \"\"\"\n",
    "        从查询中提取可能的菜品名称\n",
    "        简单实现，实际应用中可能需要更复杂的NLP方法\n",
    "        \n",
    "        Args:\n",
    "            query: 查询文本\n",
    "            \n",
    "        Returns:\n",
    "            可能的菜品列表\n",
    "        \"\"\"\n",
    "        # 这里使用简单的方法，假设查询中\"点xxx\"或\"要xxx\"中的xxx是菜品\n",
    "        dishes = []\n",
    "        patterns = [r'点([^，。!?]+)', r'要([^，。!?]+)']\n",
    "        \n",
    "        for pattern in patterns:\n",
    "            matches = re.findall(pattern, query)\n",
    "            dishes.extend(matches)\n",
    "        \n",
    "        return dishes\n",
    "    \n",
    "    def calculate_total_score(self, asr_score, dom_score, que_score):\n",
    "        \"\"\"\n",
    "        计算总得分\n",
    "        \n",
    "        Args:\n",
    "            asr_score: ASR得分\n",
    "            dom_score: DOM得分\n",
    "            que_score: QUE得分\n",
    "            \n",
    "        Returns:\n",
    "            总得分\n",
    "        \"\"\"\n",
    "        total_score = (\n",
    "            self.weights[0] * asr_score + \n",
    "            self.weights[1] * dom_score + \n",
    "            self.weights[2] * que_score\n",
    "        )\n",
    "        return total_score\n",
    "    \n",
    "    def evaluate(self, test_data):\n",
    "        \"\"\"\n",
    "        评估完整测试数据\n",
    "        \n",
    "        Args:\n",
    "            test_data: 测试数据，包含ASR、DOM和QUE的评估数据\n",
    "            \n",
    "        Returns:\n",
    "            各项得分和总分\n",
    "        \"\"\"\n",
    "        # 评估ASR\n",
    "        asr_score = self.evaluate_asr(\n",
    "            test_data['asr_references'], \n",
    "            test_data['asr_hypotheses']\n",
    "        )\n",
    "        \n",
    "        # 评估DOM\n",
    "        dom_score = self.evaluate_dom(\n",
    "            test_data['dom_true_labels'], \n",
    "            test_data['dom_predicted_labels']\n",
    "        )\n",
    "        \n",
    "        # 评估QUE\n",
    "        que_score = self.evaluate_que(\n",
    "            test_data['que_queries'], \n",
    "            test_data['dom_predicted_labels'],  # 使用DOM的预测结果\n",
    "            test_data['que_model_scores']\n",
    "        )\n",
    "        \n",
    "        # 计算总分\n",
    "        total_score = self.calculate_total_score(asr_score, dom_score, que_score)\n",
    "        \n",
    "        return {\n",
    "            'asr_score': asr_score,\n",
    "            'dom_score': dom_score,\n",
    "            'que_score': que_score,\n",
    "            'total_score': total_score\n",
    "        }\n",
    "\n",
    "\n",
    "# 使用示例\n",
    "def main():\n",
    "    # 初始化评估器\n",
    "    evaluator = ELEAIEvaluator()\n",
    "    \n",
    "    # 加载菜品数据库\n",
    "    evaluator.load_dish_database('../data/商品.xlsx')\n",
    "    \n",
    "    # 准备测试数据\n",
    "    test_data = {\n",
    "        # ASR测试数据\n",
    "        'asr_references': [\n",
    "            \"我在减肥，帮我点个不长胖的外卖\",\n",
    "            \"给我来个酸辣土豆丝，不要辣的\",\n",
    "            \"我想吃麻辣烫，要特别辣的那种\"\n",
    "        ],\n",
    "        'asr_hypotheses': [\n",
    "            \"我在减肥帮我点个不长胖的外卖\",\n",
    "            \"给我来个酸辣土豆丝不要辣的\",\n",
    "            \"我想吃麻辣烫要特别辣的那种\"\n",
    "        ],\n",
    "        \n",
    "        # DOM测试数据\n",
    "        'dom_true_labels': [1, 1, 1],\n",
    "        'dom_predicted_labels': [1, 1, 0],\n",
    "        \n",
    "        # QUE测试数据\n",
    "        'que_queries': [\n",
    "            \"帮我点沙拉\",\n",
    "            \"帮我点酸辣土豆丝\",\n",
    "            \"帮我点麻辣烫\"\n",
    "        ],\n",
    "        'que_model_scores': [4, 5, 3]  # 模型给出的评分(0-5)\n",
    "    }\n",
    "    \n",
    "    # 进行评估\n",
    "    results = evaluator.evaluate(test_data)\n",
    "    \n",
    "    # 打印结果\n",
    "    print(\"\\n===== 评估结果 =====\")\n",
    "    print(f\"ASR得分: {results['asr_score']:.4f}\")\n",
    "    print(f\"DOM得分: {results['dom_score']:.4f}\")\n",
    "    print(f\"QUE得分: {results['que_score']:.4f}\")\n",
    "    print(f\"总分: {results['total_score']:.4f}\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35d6e333-9b50-44a6-b08e-180b6d9574fe",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "langchain",
   "language": "python",
   "name": "langchain"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
